---
title: "[TIL] 2025. 4. 16"
date: 2025-04-16
categories:
  - TIL
tags:
  - TIL
  - 내일배움캠프
toc: true
toc_sticky: true
---
## 머신러닝 특강 - 이상탐지

## 머신러닝 특강 - 회귀

### 1. 회귀란?
- 연속형의 결과 값을 예측하는 기법
- 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법
- 1개 이상의 독립 변수와 종속 변수 간의 관계를 모델링
- 분류와의 차이점: 결과 값이 연속형

#### 회귀모델의 기본원리
$y = β₀ + β₁x + ε$
오차를 최소화하는 회귀계수를 찾기

#### 비용함수(Cost Function) ( \fallingdotseq 손실함수)
오차에 대한 식, 전체 데이터셋에 대한 오차의 평균 또는 합
회귀모델의 목표 → 오차 최소화 = 비용함수의 값을 최소화
- MSE(Mean Squared Error) - 오차를 제곱한 것의 평균
	$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$
- MAE(Mean Absolute Error) - 오차 절댓값의 평균
	$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|​$

비용함수를 최소화 하는 방식
1. 최소자승법 (Ordinary Least Squares) : 수학적으로 풀어서 직접 최적해 계산
	- 미분을 통해 잔차제곱합 $\text{RSS} = \sum_{i=1}^{n} \left(y_i - (\beta_0 + \beta_1 x_i)\right)^2$을 최소화하는 $\beta_0, \beta_1$ 을 구함.
	- 모델 : 기본적인 선형회귀모델
2. 경사하강법(Gradient Descent) : 경험적으로 오차를 최소화하는 계수 찾기
	- 원리 
		- - step1: $\beta_0, \beta_1$를 임의의 값으로 설정하고, 첫 비용함수의 값을 계산함.
		- step2: $\beta_0, \beta_1$ 를 새로운 값으로 업데이트 한 후 다시 비용함수의 값 계산
		- step3: 비용함수 값이 감소했으면 다시 step2를 반복. 더 이상 비용함수의 값이 감소하지 않을 때의 $\beta_0, \beta_1$를 구하고 반복을 중지함.
	- 모델 : 대부분의 회귀모델

### 2. 선형 회귀(Linear Regression)
회귀 계수가 선형인 회귀(즉, 회귀 계수가 제곱이 아닌 상수 형태)
#### (1) 단순 선형회귀
독립변수가 1개일 때 모델

#### (2) 다중 선형회귀
2개 이상의 독립변수를 사용하는 회귀 모델
$y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε$

#### (3) 다항 회귀(Polynomial Regression)
입력 변수를 다항식으로 확장한 회귀 모델

#### 회귀 평가 지표
| 지표                        | 설명                                                                                                                                                           | 수식                                                                                      |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------- |
| MAE (Mean Absolute Error) | 오차(실제값과 예측값의 차이)를 절댓값으로 변환해 평균한 것                                                                                                                            | $\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \|y_i - \hat{y}_i\|​$                          |
| MSE (Mean Squared Error)  | 오차(실제값과 예측값의 차이)를 제곱해 평균한 것                                                                                                                                  | $\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$                           |
| RMSE (Root MSE)           | MSE 값은 오류의 제곱을 구하므로 실제 오류 평균보다 더 커지는 특성이 있어 루트를 씌운 것                                                                                                         | $\text{RMSE} =\sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$                    |
| $R^2$ (결정 계수)             | 실제값의 분산 대비 예측값의 분산 비율 1에 가까울수록 정확도가 높음 R^2=1: 완벽한 예측 (모든 예측값이 실제값과 일치) R^2=0: 모델이 평균값만큼 맞춤 (예측값이 평균 수준에서 벗어나지 않음) R^2<0: 모델이 평균으로 예측하는 것보다도 못함 (모델이 나쁘다는 신호) | $R^2 = \frac{\sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$ |

### 3. 다항회귀
2, 3차 방정식과 같은 다항식으로 표현되는 회귀
$y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ε$

#### 과적합과 과소적합
- 과적합: 훈련 데이터가 지나치게 반영되어 새로운 데이터에 대한 예측력이 떨어짐(훈련 데이터 지표는 높으나 검증 데이터 지표는 낮음)
- 과소적합: 훈련 데이터 학습도 제대로 되지 않는 상태(훈련 데이터 지표, 검증 데이터 지표 모두 낮음)

#### 과적합의 원인
- 학습 데이터가 실제 데이터를 충분히 대표하지 않는 경우
- 변수가 지나치게 많아서 모델이 필요 이상으로 복잡한 경우

### 4. 규제(Regulariztion)
과적합을 방지하기 위해 패널티를 추가해 비용함수를 조정하는 것

#### (1) 릿지 회귀(Ridge, L2)
$\text{Loss} = \sum(y_i - \hat{y}i)^2 + \lambda \sum\beta_j^2$
비용함수에 가중치의 제곱합($λΣβᵢ²$)을 패널티로 추가
효과: 가중치 크기를 줄임 (계수는 0이 되지 않음)

#### (2) 라쏘 회귀(Lasso, L1)
$\text{Loss} = \sum(y_i - \hat{y}i)^2 + \lambda \sum|\beta_j|$
비용함수에 가중치 절댓값합(λΣ|βᵢ|) 항 추가
효과: 가중치를 0으로 만들어 변수 선택(Feature Selection) 효과

#### (3) 엘라스틱넷(ElasticNet)
$\text{Loss} = \sum_{i=1}^{n}(y_i - \hat{y}i)^2 + \lambda \left( \alpha \sum|\beta_j| + (1 - \alpha) \sum\beta_j^2 \right)$
L1 + L2를 혼합
 - **하이퍼파라미터 두 개**: `λ` (정규화 세기), `α` (L1과 L2 비율)
 - 효과: 라쏘의 변수 선택 + 릿지의 안정성을 동시에 가져감
	 - 변수 선택도 가능하고(L1), 계수 축소도 가능(L2).

### 5. 비선형회귀
#### (1) 회귀 트리(Decision Tree Regressor)
- 데이터 분할 기반 예측
- 해석은 쉬우나 과적합 가능성 높음

#### (2) 앙상블 모델
- 랜덤 포레스트(Random Forest Regressor)
	여러 결정 트리를 평균하여 예측
- 그래디언트 부스팅(XGBoost, LightGBM, Gradient Boosting)
	순차적으로 모델을 보완하며 예측 정확도 향상